# AI英语工作助手 - 中期扩展技术方案

## 概述

本文档定义了AI英语工作助手从MVP到中期扩展阶段的技术升级方案。适用于**1K-50K用户规模**，重点解决性能瓶颈、可靠性提升和功能扩展，确保用户无感知平滑升级。

## 技术架构演进

### 升级前后对比

| 组件 | MVP阶段 | 中期扩展 | 升级策略 |
|------|---------|----------|----------|
| 数据库 | SQLite | PostgreSQL + 读写分离 | 数据迁移 + 连接切换 |
| 缓存 | 单Redis | Redis集群 | 渐进式迁移 |
| 向量DB | 单Qdrant | Qdrant集群 | 数据同步 + 切换 |
| 存储 | MinIO单节点 | 云存储 + CDN | 文件迁移 + 双写 |
| Agent框架 | CrewAI | CrewAI + LangGraph | 渐进式引入LangGraph |
| 后端 | 单体服务 | 微服务 + 网关 | 服务拆分 + 路由 |
| 部署 | 单机部署 | 容器集群 | 蓝绿部署 |

### 新架构图

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   浏览器插件     │    │   桌面应用       │    │   移动端App     │
│  (Chrome Ext)   │    │  (Electron)     │    │ (React Native)  │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                       │                       │
         └───────────────────────┼───────────────────────┘
                                 │
                    ┌─────────────────┐
                    │   CDN + 负载均衡 │
                    │  (Cloudflare)   │
                    └─────────────────┘
                                 │
                    ┌─────────────────┐
                    │   API网关       │
                    │  (Kong/Traefik) │
                    └─────────────────┘
                                 │
        ┌────────────────────────┼────────────────────────┐
        │                       │                        │
┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│  用户服务    │    │  翻译服务    │    │  词汇服务    │    │  Agent服务  │
│ (User API)  │    │(Translate)  │    │(Vocabulary) │    │(CrewAI+LG)  │
└─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘
        │                       │                        │
        └───────────────────────┼────────────────────────┘
                                │
        ┌────────────────────────┼────────────────────────┐
        │                       │                        │
┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│ PostgreSQL  │    │ Redis集群   │    │ Qdrant集群  │    │  云存储+CDN │
│ (主从复制)   │    │ (3节点)     │    │ (3节点)     │    │ (OSS/S3)    │
└─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘
```

## 核心升级策略

### 1. Agent框架升级：CrewAI → CrewAI + LangGraph

#### 升级策略

**第一阶段：保持CrewAI为主，引入LangGraph**
```python
# agents/hybrid_agent_system.py
from crewai import Agent, Task, Crew
from langgraph import StateGraph, END
from typing import Dict, Any, TypedDict

class AgentState(TypedDict):
    text: str
    context: str
    translation: str
    grammar_check: str
    learning_plan: str
    current_step: str

class HybridAgentSystem:
    def __init__(self):
        # 保持CrewAI agents
        self.crew_agents = self._setup_crew_agents()
        # 引入LangGraph workflow
        self.workflow = self._setup_langgraph_workflow()
    
    def _setup_langgraph_workflow(self):
        """设置LangGraph工作流"""
        workflow = StateGraph(AgentState)
        
        # 定义节点
        workflow.add_node("translate", self._translate_node)
        workflow.add_node("grammar_check", self._grammar_check_node)
        workflow.add_node("learning_plan", self._learning_plan_node)
        
        # 定义边
        workflow.set_entry_point("translate")
        workflow.add_edge("translate", "grammar_check")
        workflow.add_edge("grammar_check", "learning_plan")
        workflow.add_edge("learning_plan", END)
        
        return workflow.compile()
    
    def _translate_node(self, state: AgentState) -> AgentState:
        """翻译节点 - 使用CrewAI"""
        task = Task(
            description=f"翻译：{state['text']}",
            agent=self.crew_agents['translator']
        )
        crew = Crew(agents=[self.crew_agents['translator']], tasks=[task])
        result = crew.kickoff()
        
        state['translation'] = result
        state['current_step'] = 'translated'
        return state
    
    def process_complex_workflow(self, text: str, context: str) -> Dict[str, Any]:
        """处理复杂工作流 - 使用LangGraph编排"""
        initial_state = {
            "text": text,
            "context": context,
            "translation": "",
            "grammar_check": "",
            "learning_plan": "",
            "current_step": "start"
        }
        
        final_state = self.workflow.invoke(initial_state)
        return final_state
```

**第二阶段：逐步迁移复杂逻辑到LangGraph**
```python
# agents/advanced_langgraph_system.py
from langgraph import StateGraph, END
from langgraph.prebuilt import ToolExecutor
from typing import Dict, Any, List

class AdvancedLangGraphSystem:
    def __init__(self):
        self.workflow = self._create_advanced_workflow()
    
    def _create_advanced_workflow(self):
        """创建高级工作流"""
        workflow = StateGraph(AgentState)
        
        # 添加条件分支
        workflow.add_node("analyze_complexity", self._analyze_complexity)
        workflow.add_node("simple_translation", self._simple_translation)
        workflow.add_node("complex_translation", self._complex_translation)
        workflow.add_node("quality_check", self._quality_check)
        
        # 条件路由
        workflow.set_entry_point("analyze_complexity")
        workflow.add_conditional_edges(
            "analyze_complexity",
            self._route_by_complexity,
            {
                "simple": "simple_translation",
                "complex": "complex_translation"
            }
        )
        
        return workflow.compile()
    
    def _route_by_complexity(self, state: AgentState) -> str:
        """根据复杂度路由"""
        text_length = len(state['text'])
        if text_length < 100:
            return "simple"
        return "complex"
```

### 2. 数据库升级：SQLite → PostgreSQL

#### 升级步骤

**第一阶段：双写准备**
```python
# 数据库适配器
import os
import asyncio
from typing import Dict, Any
from sqlalchemy.ext.asyncio import AsyncSession

class DatabaseAdapter:
    def __init__(self, sqlite_session: AsyncSession, postgres_session: AsyncSession):
        self.sqlite = sqlite_session
        self.postgres = postgres_session
        self.migration_mode = os.getenv('MIGRATION_MODE', 'sqlite')

    async def create(self, model_class, data: Dict[str, Any]):
        if self.migration_mode == 'dual-write':
            # 双写模式：同时写入两个数据库
            sqlite_instance = model_class(**data)
            postgres_instance = model_class(**data)
            
            self.sqlite.add(sqlite_instance)
            self.postgres.add(postgres_instance)
            
            await self.sqlite.commit()
            await self.postgres.commit()
            
            return sqlite_instance  # 主库结果
        
        session = self.postgres if self.migration_mode == 'postgres' else self.sqlite
        instance = model_class(**data)
        session.add(instance)
        await session.commit()
        return instance
```

**第二阶段：数据迁移**
```bash
#!/bin/bash
# 数据迁移脚本

# 1. 导出SQLite数据
sqlite3 app.db ".dump" > sqlite_dump.sql

# 2. 转换为PostgreSQL格式
python convert_sqlite_to_postgres.py sqlite_dump.sql > postgres_dump.sql

# 3. 导入PostgreSQL
psql -h localhost -U app_user -d app_db < postgres_dump.sql

# 4. 验证数据一致性
node verify_migration.js
```

**第三阶段：切换读取**
```python
# 渐进式读取切换
import os
import random
import logging
from typing import Any, Optional
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

class ReadSwitcher:
    def __init__(self, sqlite_session: AsyncSession, postgres_session: AsyncSession):
        self.sqlite = sqlite_session
        self.postgres = postgres_session
        self.postgres_read_ratio = float(os.getenv('POSTGRES_READ_RATIO', '0'))
        self.logger = logging.getLogger(__name__)

    async def read(self, model_class, query_filter) -> Optional[Any]:
        if random.random() < self.postgres_read_ratio:
            try:
                result = await self.postgres.execute(
                    select(model_class).where(query_filter)
                )
                return result.scalar_one_or_none()
            except Exception as error:
                self.logger.warning(f'PostgreSQL read failed, fallback to SQLite: {error}')
                result = await self.sqlite.execute(
                    select(model_class).where(query_filter)
                )
                return result.scalar_one_or_none()
        
        result = await self.sqlite.execute(
            select(model_class).where(query_filter)
        )
        return result.scalar_one_or_none()
```

#### PostgreSQL配置

**主从复制配置**
```yaml
# docker-compose.yml
version: '3.8'
services:
  postgres-master:
    image: postgres:15
    environment:
      POSTGRES_DB: app_db
      POSTGRES_USER: app_user
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_REPLICATION_USER: replicator
      POSTGRES_REPLICATION_PASSWORD: ${REPL_PASSWORD}
    volumes:
      - ./postgres/master.conf:/etc/postgresql/postgresql.conf
      - postgres_master_data:/var/lib/postgresql/data
    command: postgres -c config_file=/etc/postgresql/postgresql.conf

  postgres-slave:
    image: postgres:15
    environment:
      PGUSER: app_user
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_MASTER_SERVICE: postgres-master
    volumes:
      - ./postgres/slave.conf:/etc/postgresql/postgresql.conf
      - postgres_slave_data:/var/lib/postgresql/data
    depends_on:
      - postgres-master
```

**性能优化配置**
```sql
-- postgresql.conf 关键配置
shared_buffers = 256MB
effective_cache_size = 1GB
work_mem = 4MB
maintenance_work_mem = 64MB
wal_buffers = 16MB
checkpoint_completion_target = 0.9
random_page_cost = 1.1

-- 索引优化
CREATE INDEX CONCURRENTLY idx_users_email_hash ON users USING hash(email);
CREATE INDEX CONCURRENTLY idx_translations_user_created_btree ON translations(user_id, created_at DESC);
CREATE INDEX CONCURRENTLY idx_vocabulary_review_gin ON vocabulary USING gin(review_data);
```

### 2. 缓存升级：单Redis → Redis集群

#### 集群配置
```yaml
# redis-cluster.yml
version: '3.8'
services:
  redis-node-1:
    image: redis:7-alpine
    command: redis-server /etc/redis/redis.conf --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --appendonly yes
    volumes:
      - ./redis/redis.conf:/etc/redis/redis.conf
      - redis_node_1:/data
    ports:
      - "7001:6379"
      - "17001:16379"

  redis-node-2:
    image: redis:7-alpine
    command: redis-server /etc/redis/redis.conf --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --appendonly yes
    volumes:
      - ./redis/redis.conf:/etc/redis/redis.conf
      - redis_node_2:/data
    ports:
      - "7002:6379"
      - "17002:16379"

  redis-node-3:
    image: redis:7-alpine
    command: redis-server /etc/redis/redis.conf --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --appendonly yes
    volumes:
      - ./redis/redis.conf:/etc/redis/redis.conf
      - redis_node_3:/data
    ports:
      - "7003:6379"
      - "17003:16379"
```

#### 渐进式迁移
```python
import os
import random
import asyncio
from typing import Optional
import redis.asyncio as redis
from rediscluster import RedisCluster

class CacheMigrator:
    def __init__(self):
        self.old_redis = redis.Redis(host='localhost', port=6379)
        self.new_cluster = RedisCluster(
            startup_nodes=[
                {'host': 'localhost', 'port': 7001},
                {'host': 'localhost', 'port': 7002},
                {'host': 'localhost', 'port': 7003}
            ],
            decode_responses=True
        )
        self.migration_ratio = float(os.getenv('CACHE_MIGRATION_RATIO', '0'))

    async def get(self, key: str) -> Optional[str]:
        if random.random() < self.migration_ratio:
            value = await self.new_cluster.get(key)
            if value:
                return value
        return await self.old_redis.get(key)

    async def set(self, key: str, value: str, ttl: int) -> None:
        # 双写策略
        tasks = [self.old_redis.setex(key, ttl, value)]
        if self.migration_ratio > 0:
            tasks.append(self.new_cluster.setex(key, ttl, value))
        
        await asyncio.gather(*tasks, return_exceptions=True)
```

### 3. 向量数据库升级：单Qdrant → Qdrant集群

#### 集群部署
```yaml
# qdrant-cluster.yml
version: '3.8'
services:
  qdrant-node-1:
    image: qdrant/qdrant:v1.7.0
    environment:
      QDRANT__CLUSTER__ENABLED: true
      QDRANT__CLUSTER__P2P__PORT: 6335
      QDRANT__CLUSTER__CONSENSUS__TICK_PERIOD_MS: 100
    volumes:
      - qdrant_node_1:/qdrant/storage
    ports:
      - "6333:6333"
      - "6334:6334"
      - "6335:6335"

  qdrant-node-2:
    image: qdrant/qdrant:v1.7.0
    environment:
      QDRANT__CLUSTER__ENABLED: true
      QDRANT__CLUSTER__P2P__PORT: 6335
      QDRANT__CLUSTER__CONSENSUS__TICK_PERIOD_MS: 100
      QDRANT__CLUSTER__BOOTSTRAP__PEER: qdrant-node-1:6335
    volumes:
      - qdrant_node_2:/qdrant/storage
    ports:
      - "6343:6333"
      - "6344:6334"
      - "6345:6335"
    depends_on:
      - qdrant-node-1

  qdrant-node-3:
    image: qdrant/qdrant:v1.7.0
    environment:
      QDRANT__CLUSTER__ENABLED: true
      QDRANT__CLUSTER__P2P__PORT: 6335
      QDRANT__CLUSTER__CONSENSUS__TICK_PERIOD_MS: 100
      QDRANT__CLUSTER__BOOTSTRAP__PEER: qdrant-node-1:6335
    volumes:
      - qdrant_node_3:/qdrant/storage
    ports:
      - "6353:6333"
      - "6354:6334"
      - "6355:6335"
    depends_on:
      - qdrant-node-1
```

#### 数据迁移策略
```python
import asyncio
import logging
from typing import List, Dict, Any
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, CreateCollection

class VectorMigrator:
    def __init__(self):
        self.logger = logging.getLogger(__name__)

    async def migrate_collection(self, collection_name: str) -> None:
        old_client = QdrantClient(host='old-qdrant', port=6333)
        new_client = QdrantClient(
            host='qdrant-cluster',
            port=6333,
            # 集群负载均衡
            # nodes=[
            #     {'host': 'qdrant-node-1', 'port': 6333},
            #     {'host': 'qdrant-node-2', 'port': 6343},
            #     {'host': 'qdrant-node-3', 'port': 6353}
            # ]
        )

        # 1. 创建新集合（分片配置）
        await new_client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=768, distance=Distance.COSINE),
            shard_number=3,
            replication_factor=2
        )

        # 2. 批量迁移数据
        offset = 0
        batch_size = 1000
        
        while True:
            points, _ = await old_client.scroll(
                collection_name=collection_name,
                limit=batch_size,
                offset=offset
            )
            
            if not points:
                break
            
            await new_client.upsert(
                collection_name=collection_name,
                points=points
            )
            
            offset += batch_size
            self.logger.info(f'Migrated {offset} points')
```

### 4. 微服务拆分

#### 服务拆分策略

**用户服务 (User Service)**
```python
# user-service/src/main.py
from fastapi import FastAPI, Depends
from fastapi.middleware.cors import CORSMiddleware
from .controllers.user_controller import UserController
from .middleware.auth_middleware import AuthMiddleware

app = FastAPI(title="User Service")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

user_controller = UserController()
auth_middleware = AuthMiddleware()

app.include_router(
    user_controller.router,
    prefix="/api/users",
    dependencies=[Depends(auth_middleware.verify_token)]
)
app.include_router(
    user_controller.auth_router,
    prefix="/api/auth"
)
```

**翻译服务 (Translation Service)**
```python
# translation-service/src/main.py
from fastapi import FastAPI, Depends
from fastapi.middleware.cors import CORSMiddleware
from .controllers.translation_controller import TranslationController
from .middleware.rate_limit_middleware import RateLimitMiddleware

app = FastAPI(title="Translation Service")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

translation_controller = TranslationController()
rate_limit_middleware = RateLimitMiddleware()

app.include_router(
    translation_controller.router,
    prefix="/api/translate",
    dependencies=[Depends(rate_limit_middleware.check_rate_limit)]
)
app.include_router(
    translation_controller.history_router,
    prefix="/api/history"
)
```

**词汇服务 (Vocabulary Service)**
```python
# vocabulary-service/src/main.py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from .controllers.vocabulary_controller import VocabularyController
from .services.vector_search_service import VectorSearchService

app = FastAPI(title="Vocabulary Service")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

vocabulary_controller = VocabularyController()
vector_search_service = VectorSearchService()

app.include_router(
    vocabulary_controller.router,
    prefix="/api/vocabulary"
)
app.include_router(
    vocabulary_controller.review_router,
    prefix="/api/review"
)
```

#### API网关配置

**Kong配置**
```yaml
# kong.yml
_format_version: "3.0"

services:
  - name: user-service
    url: http://user-service:3001
    
  - name: translation-service
    url: http://translation-service:3002
    
  - name: vocabulary-service
    url: http://vocabulary-service:3003

routes:
  - name: user-routes
    service: user-service
    paths:
      - /api/users
      - /api/auth
    
  - name: translation-routes
    service: translation-service
    paths:
      - /api/translate
      - /api/history
    
  - name: vocabulary-routes
    service: vocabulary-service
    paths:
      - /api/vocabulary
      - /api/review

plugins:
  - name: rate-limiting
    config:
      minute: 100
      hour: 1000
      
  - name: cors
    config:
      origins:
        - "*"
      methods:
        - GET
        - POST
        - PUT
        - DELETE
      headers:
        - Accept
        - Authorization
        - Content-Type
```

### 5. 存储升级：MinIO → 云存储

#### 存储适配器
```python
import os
import asyncio
from abc import ABC, abstractmethod
from typing import Optional
from minio import Minio
from oss2 import Bucket, Auth

class IStorageService(ABC):
    @abstractmethod
    async def upload(self, file: bytes, key: str) -> str:
        pass
    
    @abstractmethod
    async def download(self, key: str) -> bytes:
        pass
    
    @abstractmethod
    async def delete(self, key: str) -> None:
        pass
    
    @abstractmethod
    def get_url(self, key: str) -> str:
        pass

class StorageAdapter(IStorageService):
    def __init__(self):
        self.minio = Minio(
            endpoint=os.getenv('MINIO_ENDPOINT'),
            access_key=os.getenv('MINIO_ACCESS_KEY'),
            secret_key=os.getenv('MINIO_SECRET_KEY'),
            secure=False
        )
        
        auth = Auth(os.getenv('OSS_ACCESS_KEY'), os.getenv('OSS_SECRET_KEY'))
        self.oss = Bucket(auth, os.getenv('OSS_ENDPOINT'), os.getenv('OSS_BUCKET'))
        
        self.use_cloud = os.getenv('USE_CLOUD_STORAGE', 'false').lower() == 'true'

    async def upload(self, file: bytes, key: str) -> str:
        if self.use_cloud:
            # 双写：同时上传到云存储和MinIO
            tasks = [
                self._upload_to_oss(key, file),
                self._upload_to_minio(key, file)
            ]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            return results[0]  # 返回云存储URL
        
        await self._upload_to_minio(key, file)
        return self._get_minio_url(key)

    def get_url(self, key: str) -> str:
        if self.use_cloud:
            return f"https://cdn.example.com/{key}"
        return self._get_minio_url(key)
    
    async def _upload_to_minio(self, key: str, file: bytes) -> str:
        # MinIO上传逻辑
        self.minio.put_object('files', key, file, len(file))
        return self._get_minio_url(key)
    
    async def _upload_to_oss(self, key: str, file: bytes) -> str:
        # OSS上传逻辑
        self.oss.put_object(key, file)
        return f"https://cdn.example.com/{key}"
    
    def _get_minio_url(self, key: str) -> str:
        return f"http://{os.getenv('MINIO_ENDPOINT')}/files/{key}"
```

#### 文件迁移策略
```python
import asyncio
import logging
from typing import AsyncIterator
from minio import Minio
from oss2 import Bucket

class FileMigrator:
    def __init__(self, minio_client: Minio, oss_bucket: Bucket):
        self.minio = minio_client
        self.oss = oss_bucket
        self.logger = logging.getLogger(__name__)

    async def migrate_to_cloud(self) -> None:
        """迁移文件到云存储"""
        objects = self.minio.list_objects('files', recursive=True)
        
        for obj in objects:
            try:
                # 下载文件
                response = self.minio.get_object('files', obj.object_name)
                file_data = response.read()
                response.close()
                response.release_conn()
                
                # 上传到云存储
                result = self.oss.put_object(obj.object_name, file_data)
                
                # 验证上传成功
                head_result = self.oss.head_object(obj.object_name)
                if head_result.content_length == len(file_data):
                    self.logger.info(f'Migrated: {obj.object_name}')
                else:
                    self.logger.error(f'Size mismatch for {obj.object_name}')
                    
            except Exception as error:
                self.logger.error(f'Failed to migrate {obj.object_name}: {error}')
```

## 部署与运维

### 1. 容器化部署

**Docker Compose编排**
```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  # API网关
  kong:
    image: kong:3.4
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: postgres-master
      KONG_PG_USER: kong
      KONG_PG_PASSWORD: ${KONG_DB_PASSWORD}
    ports:
      - "8000:8000"
      - "8443:8443"
      - "8001:8001"
    depends_on:
      - postgres-master

  # 微服务
  user-service:
    build: ./services/user-service
    environment:
      DATABASE_URL: postgresql://app_user:${DB_PASSWORD}@postgres-master:5432/app_db
      REDIS_URL: redis://redis-cluster:7001
    depends_on:
      - postgres-master
      - redis-node-1

  translation-service:
    build: ./services/translation-service
    environment:
      DATABASE_URL: postgresql://app_user:${DB_PASSWORD}@postgres-master:5432/app_db
      REDIS_URL: redis://redis-cluster:7001
      QDRANT_URL: http://qdrant-node-1:6333
    depends_on:
      - postgres-master
      - redis-node-1
      - qdrant-node-1

  vocabulary-service:
    build: ./services/vocabulary-service
    environment:
      DATABASE_URL: postgresql://app_user:${DB_PASSWORD}@postgres-master:5432/app_db
      QDRANT_URL: http://qdrant-node-1:6333
    depends_on:
      - postgres-master
      - qdrant-node-1

  # 前端
  frontend:
    build: ./frontend
    environment:
      REACT_APP_API_URL: http://kong:8000
    ports:
      - "3000:80"
    depends_on:
      - kong
```

### 2. 监控系统

**Prometheus + Grafana**
```yaml
# monitoring.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'

  grafana:
    image: grafana/grafana:latest
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    ports:
      - "3001:3000"
    depends_on:
      - prometheus

  node-exporter:
    image: prom/node-exporter:latest
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
```

**关键指标监控**
```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'user-service'
    static_configs:
      - targets: ['user-service:3001']
    metrics_path: '/metrics'
    
  - job_name: 'translation-service'
    static_configs:
      - targets: ['translation-service:3002']
    metrics_path: '/metrics'
    
  - job_name: 'vocabulary-service'
    static_configs:
      - targets: ['vocabulary-service:3003']
    metrics_path: '/metrics'
    
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
    
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
    
  - job_name: 'qdrant'
    static_configs:
      - targets: ['qdrant-node-1:6333', 'qdrant-node-2:6343', 'qdrant-node-3:6353']
    metrics_path: '/metrics'
```

### 3. 日志系统

**ELK Stack**
```yaml
# logging.yml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.10.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"

  logstash:
    image: docker.elastic.co/logstash/logstash:8.10.0
    volumes:
      - ./logging/logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    ports:
      - "5044:5044"
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:8.10.0
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.10.0
    volumes:
      - ./logging/filebeat.yml:/usr/share/filebeat/filebeat.yml
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      - logstash
```

## 性能优化

### 1. 数据库优化

**连接池配置**
```python
# database/pool.py
import os
import asyncio
from typing import List, Any, Optional
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy import text

# 主库连接
master_engine = create_async_engine(
    f"postgresql+asyncpg://app_user:{os.getenv('DB_PASSWORD')}@postgres-master:5432/app_db",
    pool_size=20,
    max_overflow=0,
    pool_timeout=2,
    pool_recycle=30
)

# 从库连接
slave_engine = create_async_engine(
    f"postgresql+asyncpg://app_user:{os.getenv('DB_PASSWORD')}@postgres-slave:5432/app_db",
    pool_size=10,
    max_overflow=0,
    pool_timeout=2,
    pool_recycle=30
)

# 会话工厂
MasterSession = sessionmaker(
    master_engine, class_=AsyncSession, expire_on_commit=False
)
SlaveSession = sessionmaker(
    slave_engine, class_=AsyncSession, expire_on_commit=False
)

class DatabaseService:
    async def query(self, sql: str, params: Optional[List[Any]] = None, read_only: bool = False) -> List[Any]:
        session_factory = SlaveSession if read_only else MasterSession
        
        async with session_factory() as session:
            try:
                result = await session.execute(text(sql), params or [])
                return result.fetchall()
            except Exception as e:
                await session.rollback()
                raise e
```

**查询优化**
```sql
-- 分区表设计
CREATE TABLE translations (
    id BIGSERIAL,
    user_id INTEGER NOT NULL,
    source_text TEXT NOT NULL,
    target_text TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
) PARTITION BY RANGE (created_at);

-- 按月分区
CREATE TABLE translations_2024_01 PARTITION OF translations
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE translations_2024_02 PARTITION OF translations
FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- 索引优化
CREATE INDEX CONCURRENTLY idx_translations_user_created 
ON translations (user_id, created_at DESC) 
WHERE created_at > CURRENT_DATE - INTERVAL '30 days';
```

### 2. 缓存策略

**多级缓存**
```python
import json
import asyncio
from typing import Any, Optional, Dict
from collections import OrderedDict
from rediscluster import RedisCluster

class CacheService:
    def __init__(self, redis_cluster: RedisCluster, max_l1_size: int = 1000):
        self.l1_cache: OrderedDict[str, Any] = OrderedDict()  # 内存缓存
        self.l2_cache = redis_cluster  # Redis集群
        self.max_l1_size = max_l1_size
    
    async def get(self, key: str) -> Optional[Any]:
        # L1缓存
        if key in self.l1_cache:
            # 更新访问顺序
            self.l1_cache.move_to_end(key)
            return self.l1_cache[key]
        
        # L2缓存
        value = await self.l2_cache.get(key)
        if value:
            parsed_value = json.loads(value)
            self.l1_cache[key] = parsed_value
            self._evict_if_needed()
            return parsed_value
        
        return None
    
    async def set(self, key: str, value: Any, ttl: int = 3600) -> None:
        # 写入L1缓存
        self.l1_cache[key] = value
        self.l1_cache.move_to_end(key)
        
        # 写入L2缓存
        await self.l2_cache.setex(key, ttl, json.dumps(value))
        
        # L1缓存大小控制
        self._evict_if_needed()
    
    def _evict_if_needed(self) -> None:
        """LRU淘汰策略"""
        while len(self.l1_cache) > self.max_l1_size:
            self.l1_cache.popitem(last=False)
```

### 3. API优化

**请求合并**
```python
import asyncio
from typing import Any, Dict, List, Callable, Awaitable
from dataclasses import dataclass
from collections import defaultdict

@dataclass
class BatchItem:
    item: Any
    future: asyncio.Future

class BatchProcessor:
    def __init__(self, batch_timeout: float = 0.1, max_batch_size: int = 10):
        self.batches: Dict[str, List[BatchItem]] = defaultdict(list)
        self.timers: Dict[str, asyncio.Task] = {}
        self.batch_timeout = batch_timeout
        self.max_batch_size = max_batch_size
    
    async def add_to_batch(self, batch_key: str, item: Any) -> Any:
        future = asyncio.Future()
        batch_item = BatchItem(item=item, future=future)
        
        self.batches[batch_key].append(batch_item)
        
        # 批量处理定时器
        if batch_key not in self.timers:
            timer_task = asyncio.create_task(
                self._schedule_batch_processing(batch_key)
            )
            self.timers[batch_key] = timer_task
        
        # 批量大小限制
        if len(self.batches[batch_key]) >= self.max_batch_size:
            if batch_key in self.timers:
                self.timers[batch_key].cancel()
            await self._process_batch(batch_key)
        
        return await future
    
    async def _schedule_batch_processing(self, batch_key: str) -> None:
        await asyncio.sleep(self.batch_timeout)
        await self._process_batch(batch_key)
    
    async def _process_batch(self, batch_key: str) -> None:
        batch = self.batches.get(batch_key, [])
        if not batch:
            return
        
        # 清理状态
        del self.batches[batch_key]
        if batch_key in self.timers:
            del self.timers[batch_key]
        
        try:
            items = [batch_item.item for batch_item in batch]
            results = await self._execute_batch(batch_key, items)
            
            for batch_item, result in zip(batch, results):
                if not batch_item.future.done():
                    batch_item.future.set_result(result)
        except Exception as error:
            for batch_item in batch:
                if not batch_item.future.done():
                    batch_item.future.set_exception(error)
    
    async def _execute_batch(self, batch_key: str, items: List[Any]) -> List[Any]:
        # 子类需要实现具体的批量处理逻辑
        raise NotImplementedError("Subclasses must implement _execute_batch")
```

## 成本估算

### 月度成本预估（1K-50K用户）

| 组件 | 1K用户 | 10K用户 | 50K用户 |
|------|--------|---------|----------|
| 云服务器 | $50 | $200 | $800 |
| 数据库 | $20 | $100 | $400 |
| 缓存 | $10 | $50 | $200 |
| 存储+CDN | $5 | $30 | $150 |
| AI模型 | $20 | $200 | $1000 |
| 监控日志 | $10 | $30 | $100 |
| **总计** | **$115** | **$610** | **$2650** |

### 成本优化策略

1. **智能缓存**: 减少AI API调用
2. **数据压缩**: 降低存储成本
3. **CDN优化**: 减少带宽费用
4. **资源调度**: 按需扩缩容
5. **预留实例**: 长期折扣

## 升级时间表

### 第1-2周：基础设施准备
- [ ] PostgreSQL集群部署
- [ ] Redis集群配置
- [ ] Qdrant集群搭建
- [ ] 监控系统部署

### 第3-4周：数据迁移
- [ ] 数据库双写模式
- [ ] SQLite数据迁移
- [ ] 缓存数据迁移
- [ ] 向量数据迁移

### 第5-6周：服务拆分
- [ ] 微服务开发
- [ ] API网关配置
- [ ] 服务间通信
- [ ] 负载测试

### 第7-8周：切换上线
- [ ] 灰度发布
- [ ] 流量切换
- [ ] 性能监控
- [ ] 问题修复

## 风险控制

### 1. 技术风险
- **数据一致性**: 双写验证 + 数据校验
- **服务可用性**: 熔断降级 + 自动恢复
- **性能下降**: 压力测试 + 性能监控

### 2. 业务风险
- **用户体验**: 灰度发布 + 快速回滚
- **数据安全**: 备份策略 + 访问控制
- **成本控制**: 预算告警 + 资源优化

## 总结

中期扩展方案通过**渐进式升级**策略，确保从MVP到中等规模的平滑过渡。重点关注数据迁移的无损性、服务拆分的渐进性和性能优化的持续性，为下一阶段的大规模扩展奠定坚实基础。